

- implement accumulating gradients

- fix error

- make it reproducible (added seeds for learner and actor)

TODO:

- debug (today)

- batched actor (for inference and multiple env) (today)

- start main training (today)








- investigate how epsilon and random action slows learning (bug is not epsilon)

    (if model is determined to get alloc to 1, then random action wont do anything
     since any changes will be negated by model and alloc will still be 1)

    (problem can be solved with batched env, where each env has different epsilon)

    (if epsilon works, then batched env with variable epsilon is a requirement)

    (possible explanation for why loss goes up as model tries different allocs, forcing critic distribution to widen)

